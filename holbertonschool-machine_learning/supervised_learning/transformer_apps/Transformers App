{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPo0NF3J9jzOPD4mwG3Z3SY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobabyyy/holbertonschool-machine_learning/blob/main/holbertonschool-machine_learning/supervised_learning/transformer_apps/Transformers%20App\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"Transformer App: Dataset\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    class Dataset that loads and preps a dataset for machine translation:\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        data_train: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset train split,\n",
        "                    loaded as_supervided\n",
        "        data_valid: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset validate split,\n",
        "                    loaded as_supervided\n",
        "        tokenizer_pt: Portuguese tokenizer created from\n",
        "                      the training set\n",
        "        tokenizer_en: English tokenizer created\n",
        "                      from the training set\n",
        "        \"\"\"\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "\n",
        "        # Create tokenizers\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        this function's purpose is to build a tokenizer that can convert portuguese\n",
        "        sentences into sequences of tokens, these tokens are typically numerical.\n",
        "        \"\"\"\n",
        "        tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (pt.numpy() for pt, en in data),\n",
        "            target_vocab_size=2**15\n",
        "        )\n",
        "\n",
        "        # build English tokenizer from the training set\n",
        "        tokenizer_en = (\n",
        "            tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (en.numpy() for pt, en in data),\n",
        "            target_vocab_size=2**15\n",
        "        ))\n",
        "\n",
        "        return tokenizer_pt, tokenizer_en\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  import tensorflow as tf\n",
        "\n",
        "  data = Dataset()\n",
        "  for pt, en in data.data_train.take(1):\n",
        "      print(pt.numpy().decode('utf-8'))\n",
        "      print(en.numpy().decode('utf-8'))\n",
        "  for pt, en in data.data_valid.take(1):\n",
        "      print(pt.numpy().decode('utf-8'))\n",
        "      print(en.numpy().decode('utf-8'))\n",
        "  print(type(data.tokenizer_pt))\n",
        "  print(type(data.tokenizer_en))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbgYtj-qk9C8",
        "outputId": "771cb998-83b1-4c29-fbcd-5944c1d58850"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "tinham comido peixe com batatas fritas ?\n",
            "did they eat fish and chips ?\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Transformers App -\n",
        "Taks 1: Encode Tokens\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Class that loads and preps a dataset for machine translation:\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        data_train: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset train split,\n",
        "                    loaded as_supervided\n",
        "        data_valid: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset validate split,\n",
        "                    loaded as_supervided\n",
        "        tokenizer_pt: Portuguese tokenizer created from\n",
        "                      the training set\n",
        "        tokenizer_en: English tokenizer created\n",
        "                      from the training set\n",
        "        \"\"\"\n",
        "\n",
        "        # load training and validation\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "\n",
        "        # build subword tokenizers for Portuguese and English\n",
        "        self.tokenizer_pt = (\n",
        "            tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (pt.numpy() for pt, en in self.data_train),\n",
        "            target_vocab_size=2**15\n",
        "        ))\n",
        "        self.tokenizer_en = (\n",
        "            tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (en.numpy() for pt, en in self.data_train),\n",
        "            target_vocab_size=2**15\n",
        "        ))\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        # retrieve the tokenizers created during init\n",
        "        tokenizer_pt = self.tokenizer_pt\n",
        "        tokenizer_en = self.tokenizer_en\n",
        "\n",
        "        def encode(pt, en):\n",
        "            # tokenize sentences\n",
        "            pt_tokens = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(pt.numpy()) + [tokenizer_pt.vocab_size]\n",
        "            en_tokens = [tokenizer_en.vocab_size] + tokenizer_en.encode(en.numpy()) + [tokenizer_en.vocab_size]\n",
        "\n",
        "            return np.array(pt_tokens), np.array(en_tokens)\n",
        "\n",
        "        # apply encode function to each element of dataset\n",
        "        data = data.map(encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        # retrieve the tokenizers created during init\n",
        "        tokenizer_pt = self.tokenizer_pt\n",
        "        tokenizer_en = self.tokenizer_en\n",
        "\n",
        "    def encode(self, pt, en):\n",
        "        # use the tokenizer instances to encode the sentences\n",
        "        pt_tokens = self.tokenizer_pt.encode(pt.numpy())\n",
        "        en_tokens = self.tokenizer_en.encode(en.numpy())\n",
        "\n",
        "        # add start and end tokens\n",
        "        pt_tokens = [self.tokenizer_pt.vocab_size] + pt_tokens + [self.tokenizer_pt.vocab_size + 1]\n",
        "        en_tokens = [self.tokenizer_en.vocab_size] + en_tokens + [self.tokenizer_en.vocab_size + 1]\n",
        "\n",
        "        return np.array(pt_tokens), np.array(en_tokens)\n",
        "\n",
        "# testing our output\n",
        "if __name__ == \"__main__\":\n",
        "  import tensorflow as tf\n",
        "\n",
        "  data = Dataset()\n",
        "  for pt, en in data.data_train.take(1):\n",
        "      print(data.encode(pt, en))\n",
        "  for pt, en in data.data_valid.take(1):\n",
        "      print(data.encode(pt, en))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki1qTbvnZop",
        "outputId": "182c1253-805e-4ed1-9feb-6db9f65c4c1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([30138,     6,    36, 17925,    13,     3,  3037,     1,  4880,\n",
            "           3,   387,  2832,    18, 18444,     1,     5,     8,     3,\n",
            "       16679, 19460,   739,     2, 30139]), array([28543,     4,    56,    15,  1266, 20397, 10721,     1,    15,\n",
            "         100,   125,   352,     3,    45,  3066,     6,  8004,     1,\n",
            "          88,    13, 14859,     2, 28544]))\n",
            "(array([30138,   289, 15409,  2591,    19, 20318, 26024, 29997,    28,\n",
            "       30139]), array([28543,    93,    25,   907,  1366,     4,  5742,    33, 28544]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#usr/bin/env python3\n",
        "\"\"\"\n",
        "Transformer App -\n",
        "Task 2: TF Encode.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Class that loads and preps a dataset for machine translation:\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        data_train: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset train split,\n",
        "                    loaded as_supervided\n",
        "        data_valid: contains the ted_hrlr_translate/\n",
        "                    pt_to_en tf.data.Dataset validate split,\n",
        "                    loaded as_supervided\n",
        "        tokenizer_pt: Portuguese tokenizer created from\n",
        "                      the training set\n",
        "        tokenizer_en: English tokenizer created\n",
        "                      from the training set\n",
        "        \"\"\"\n",
        "\n",
        "        # load training and validation sets\n",
        "        data_train_raw = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        data_valid_raw = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "\n",
        "        # build subword tokenizers for Portuguese and English\n",
        "        self.tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (pt.numpy() for pt, en in data_train_raw),\n",
        "            target_vocab_size=2**15\n",
        "        )\n",
        "        self.tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (en.numpy() for pt, en in data_train_raw),\n",
        "            target_vocab_size=2**15\n",
        "        )\n",
        "\n",
        "        # tokenize datasets\n",
        "        self.data_train = data_train_raw.map(self.tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        self.data_valid = data_valid_raw.map(self.tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    def tf_encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        `t_encode` function is responsible for tokenziation and encoding of the text.\n",
        "        The function's purpose is to serve as a bridge between tensorflows computational\n",
        "        graph and the custom encode method.\n",
        "        \"\"\"\n",
        "        result_pt, result_en = tf.py_function(func=self.encode, inp=[pt, en], Tout=[tf.int64, tf.int64])\n",
        "        result_pt.set_shape([None])\n",
        "        result_en.set_shape([None])\n",
        "        return result_pt, result_en\n",
        "\n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        `encode` function is responsible for converting text data into tokenized and numerical\n",
        "        format.\n",
        "        Method takes 2 parameters, `pt`, `en` which rep portuguese and english, respectively.\n",
        "        \"\"\"\n",
        "        pt_tokens = [self.tokenizer_pt.vocab_size] + self.tokenizer_pt.encode(pt.numpy()) + [self.tokenizer_pt.vocab_size + 1]\n",
        "        en_tokens = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(en.numpy()) + [self.tokenizer_en.vocab_size + 1]\n",
        "        return np.array(pt_tokens), np.array(en_tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  import tensorflow as tf\n",
        "\n",
        "  data = Dataset()\n",
        "  # print('got here')\n",
        "  for pt, en in data.data_train.take(1):\n",
        "      print(pt, en)\n",
        "  for pt, en in data.data_valid.take(1):\n",
        "      print(pt, en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Vz1_KQgjhr",
        "outputId": "f4f5c75f-858f-4f22-a90c-3238e95c898c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[30138     6    36 17925    13     3  3037     1  4880     3   387  2832\n",
            "    18 18444     1     5     8     3 16679 19460   739     2 30139], shape=(23,), dtype=int64) tf.Tensor(\n",
            "[28543     4    56    15  1266 20397 10721     1    15   100   125   352\n",
            "     3    45  3066     6  8004     1    88    13 14859     2 28544], shape=(23,), dtype=int64)\n",
            "tf.Tensor([30138   289 15409  2591    19 20318 26024 29997    28 30139], shape=(10,), dtype=int64) tf.Tensor([28543    93    25   907  1366     4  5742    33 28544], shape=(9,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Transformers App -\n",
        "Task 3: Pipeline.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Class that loads and preps a dataset for machine translation.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, max_len):\n",
        "        \"\"\"\n",
        "        Update class constructor __init__:\n",
        "        batch_size: the batch size for the training/validation\n",
        "        max_len: the max num of tokens allowed per sentence\n",
        "        UPDATE: data_train attribute by performing the following actions:\n",
        "                1: filter out all examples that have either sentence w/more\n",
        "                   than max_len tokens\n",
        "                2: cache the dataset to increase performance\n",
        "                3: shuffle the entire dataset\n",
        "                4: split the dataset into padded batches of size batch_size\n",
        "                5: prefetch the dataset using tf.data.experimental.AUTOTUNE\n",
        "                   performance\n",
        "        \"\"\"\n",
        "        # load training and validation\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "\n",
        "        # build subword tokenizers for Portuguese and English\n",
        "        self.tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (pt.numpy() for pt, en in self.data_train),\n",
        "            target_vocab_size=2**15\n",
        "        )\n",
        "\n",
        "        self.tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "            (en.numpy() for pt, en in self.data_train),\n",
        "            target_vocab_size=2**15\n",
        "        )\n",
        "\n",
        "        # filter, cache, shuffle, batch, and prefetch training dataset\n",
        "        self.data_train = self.data_train.filter(lambda pt, en: tf.logical_and(tf.size(pt) <= max_len, tf.size(en) <= max_len))\n",
        "        self.data_train = self.data_train.map(self.tf_encode)\n",
        "        self.data_train = self.data_train.cache()\n",
        "        self.data_train = self.data_train.shuffle(tf.data.experimental.cardinality(self.data_train).numpy())\n",
        "        self.data_train = self.data_train.padded_batch(batch_size)\n",
        "        self.data_train = self.data_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        # filter and batch the validation dataset\n",
        "        self.data_valid = self.data_valid.filter(lambda pt, en: tf.logical_and(tf.size(pt) <= max_len, tf.size(en) <= max_len))\n",
        "        self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "        self.data_valid = self.data_valid.padded_batch(batch_size)\n",
        "\n",
        "    def tf_encode(self, pt, en):\n",
        "        pt, en = tf.py_function(self.encode, [pt, en], [tf.int64, tf.int64])\n",
        "        pt.set_shape([None])\n",
        "        en.set_shape([None])\n",
        "        return pt, en\n",
        "\n",
        "    def encode(self, pt, en):\n",
        "        pt_tokens = self.tokenizer_pt.encode(pt.numpy())\n",
        "        en_tokens = self.tokenizer_en.encode(en.numpy())\n",
        "\n",
        "        # add start and end tokens\n",
        "        pt_tokens = [self.tokenizer_pt.vocab_size] + pt_tokens + [self.tokenizer_pt.vocab_size + 1]\n",
        "        en_tokens = [self.tokenizer_en.vocab_size] + en_tokens + [self.tokenizer_en.vocab_size + 1]\n",
        "\n",
        "        return np.array(pt_tokens), np.array(en_tokens)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "data = Dataset(32, 40)\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt, en)\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt, en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOtk7yWfig3y",
        "outputId": "cc4f356e-69cd-4fb4-8f11-8f68a63be30a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[30138    26    93 ...     0     0     0]\n",
            " [30138     7 14117 ...     0     0     0]\n",
            " [30138     7 15136 ...     0     0     0]\n",
            " ...\n",
            " [30138    23    26 ...     0     0     0]\n",
            " [30138   409    38 ...     0     0     0]\n",
            " [30138 16698 29927 ...     0     0     0]], shape=(32, 51), dtype=int64) tf.Tensor(\n",
            "[[28543    11    20 ...     0     0     0]\n",
            " [28543   107     1 ...     0     0     0]\n",
            " [28543     4    91 ...     0     0     0]\n",
            " ...\n",
            " [28543    23    11 ...     0     0     0]\n",
            " [28543   425    55 ...     0     0     0]\n",
            " [28543   640 16988 ...     0     0     0]], shape=(32, 53), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[30138   289 15409 ...     0     0     0]\n",
            " [30138    86   168 ...     0     0     0]\n",
            " [30138  5036     9 ...     0     0     0]\n",
            " ...\n",
            " [30138    69    97 ...     0     0     0]\n",
            " [30138   289 19835 ...     0     0     0]\n",
            " [30138  1157 29927 ...     0     0     0]], shape=(32, 45), dtype=int64) tf.Tensor(\n",
            "[[28543    93    25 ...     0     0     0]\n",
            " [28543    11    20 ...     0     0     0]\n",
            " [28543    11  2850 ...     0     0     0]\n",
            " ...\n",
            " [28543    94   128 ...     0     0     0]\n",
            " [28543    68    25 ...     0     0     0]\n",
            " [28543    11   406 ...     0     0     0]], shape=(32, 51), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Transformer App -\n",
        "Task 4: Create Masks.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_masks(inputs, target):\n",
        "    \"\"\"\n",
        "    inputs: tf.Tensor of shape (batch_size, seq_len_in) that contains\n",
        "            the input sentence\n",
        "    target: tf.Tensor of shape (batch_size, seq_len_out) that contains\n",
        "            the target sentence\n",
        "    This function should only use tensorflow operations in order to properly\n",
        "    function in the training step\n",
        "\n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "    encoder_mask: is the tf.Tensor padding mask of shape\n",
        "                  (batch_size, 1, 1, seq_len_in) to be applied in the encoder\n",
        "    combined_mask: is the tf.Tensor of shape (batch_size, 1, seq_len_out, seq_len_out)\n",
        "                   used in the 1st attention block in the decoder to\n",
        "                   pad and mask future tokens in the input received by the decoder.\n",
        "                   It takes the maximum between a lookaheadmask and the\n",
        "                   decoder target padding mask.\n",
        "    decoder_mask: is the tf.Tensor padding mask of shape\n",
        "                  (batch_size, 1, 1, seq_len_in) used in the 2nd attention\n",
        "                  block in the decoder.\n",
        "    \"\"\"\n",
        "    # encoder padding mask\n",
        "    encoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "    encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # used in the 2nd attention block in the decoder\n",
        "    decoder_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "    decoder_mask = decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones((target.shape[1], target.shape[1])), -1, 0)\n",
        "    decoder_target_padding_mask = tf.cast(tf.math.equal(target, 0), tf.float32)\n",
        "    decoder_target_padding_mask = decoder_target_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "    combined_mask = tf.maximum(decoder_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return encoder_mask, combined_mask, decoder_mask\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  import tensorflow as tf\n",
        "\n",
        "  tf.compat.v1.set_random_seed(0)\n",
        "  data = Dataset(32, 40)\n",
        "  for inputs, target in data.data_train.take(1):\n",
        "      print(create_masks(inputs, target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WJQrHOzpMUE",
        "outputId": "32d4c011-1f23-489d-9b04-884c09c75c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 1, 1, 51), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>, <tf.Tensor: shape=(32, 1, 53, 53), dtype=float32, numpy=\n",
            "array([[[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[1., 0., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 0., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32)>, <tf.Tensor: shape=(32, 1, 1, 51), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>)\n"
          ]
        }
      ]
    }
  ]
}